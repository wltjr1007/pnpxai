{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wltjr1007/pnpxai/blob/main/tutorials/lear_tutorial_explainer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Medical Image Classification with PnPXAI (1/2)\n",
        "\n",
        "In this notebook, we demonstrate the use of the PnPXAI library for explaining a ResNet-based image classification model on the ADNI (Alzheimer's Disease Neuroimaging Initiative) dataset.\n",
        "\n",
        "**Contents:**\n",
        "1. [Setup](#setup)\n",
        "    - [Clone PnPXAI repository and install dependencies](#clone-install)\n",
        "2. [Loading Data and Model](#data-model)\n",
        "    - [Load ADNI Dataset](#load-data)\n",
        "    - [Load Pre-trained ResNet Model](#load-model)\n",
        "3. [Explanation Using PnPXAI](#explanation)\n",
        "    - [Add the LEAR Explainer](#lear-explainer)\n",
        "    - [Generate Explanations](#generate-explanations)\n",
        "4. [Visualization](#visualization)\n",
        "    - [Visualize Original Images and Explanations](#visualize-images)\n",
        "5. [Evaluation of Explanations](#evaluation)\n",
        "    - [MuFidelity](#mufidelity)\n",
        "    - [Sensitivity](#sensitivity)\n",
        "    - [Complexity](#complexity)\n",
        "\n",
        "This example demonstrates how PnPXAI can be used to gain insights into the decision-making process of a medical image classification model, specifically for Alzheimer's disease classification.\n"
      ],
      "metadata": {
        "id": "kUiAGzqlYvFI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 1. Setup<a name=\"setup\"></a>\n",
        "\n",
        "First, we clone the PnPXAI repository and install the required dependencies."
      ],
      "metadata": {
        "id": "DpkHxGXfwOon"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ccL8Vtnwrv9u"
      },
      "outputs": [],
      "source": [
        "!git clone --quiet https://github.com/wltjr1007/pnpxai\n",
        "!pip install -q -e /content/pnpxai\n",
        "!pip install -q datasets\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/pnpxai')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "AwyKIJaNr6CM"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "\n",
        "from transformers import AutoImageProcessor, ResNetForImageClassification, set_seed\n",
        "from datasets import load_dataset\n",
        "\n",
        "from pnpxai import AutoExplanationForImageClassification\n",
        "from pnpxai.explainers.grad_cam import NoCamTargetLayerAndNotTraceableError\n",
        "\n",
        "set_seed(0)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "DATA_REP = \"wltjr1007/MRI\"\n",
        "MODEL_REP = \"evanrsl/resnet-Alzheimer\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 2. Loading Data and Model<a name=\"data-model\"></a>"
      ],
      "metadata": {
        "id": "rCkK6VEJw4TD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 2.1 Load ADNI Dataset<a name=\"load-data\"></a>\n",
        "\n",
        "In this step, we load the ADNI dataset from the `datasets` library, which will be used to train and evaluate the model.\n",
        "\n",
        "\n",
        "### Preprocessing Steps\n",
        "1. **Image Transformation**: All images are converted to RGB format and preprocessed using a pre-trained image processor (e.g., ResNet-specific).\n",
        "2. **Normalization**: The pixel values are normalized using the mean and standard deviation provided by the pre-trained model configuration."
      ],
      "metadata": {
        "id": "YyXs5lokwYS1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8IBXXprv49R"
      },
      "outputs": [],
      "source": [
        "def transform(examples):\n",
        "    examples[\"pixel_values\"] = [processor(images=img.convert(\"RGB\"), return_tensors=\"pt\")[\"pixel_values\"].squeeze(0).to(device=device) for img in examples[\"image\"]]\n",
        "    examples[\"label\"] = torch.tensor(examples[\"label\"]).to(device=device)\n",
        "    del examples[\"image\"]\n",
        "    return examples\n",
        "\n",
        "processor = AutoImageProcessor.from_pretrained(MODEL_REP)\n",
        "dataset = load_dataset(DATA_REP)['test'].with_transform(transform)\n",
        "num_classes = dataset.features[\"label\"].num_classes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ADNI Dataset\n",
        "\n",
        "The **Alzheimer's Disease Neuroimaging Initiative (ADNI)** dataset is widely used for research on Alzheimer's disease (AD). It contains various types of data, including imaging data such as MRI, PET scans, and clinical assessments. This dataset enables classification and prediction of disease progression through machine learning models.\n",
        "\n",
        "### Dataset Classes\n",
        "\n",
        "The ADNI dataset is categorized into the following diagnostic labels:\n",
        "\n",
        "- **Normal**: Indicates individuals with no signs of cognitive impairment or Alzheimer's disease.\n",
        "- **MCI (Mild Cognitive Impairment)**:\n",
        "  - **sMCI (Stable MCI)**: MCI cases where the condition does not progress to AD over a specified period.\n",
        "  - **pMCI (Progressive MCI)**: MCI cases that eventually progress to AD.\n",
        "- **AD (Alzheimer's Disease)**: Represents individuals clinically diagnosed with Alzheimer's disease.\n",
        "\n",
        "Each class label corresponds to a numeric value in the dataset for ease of use in machine learning tasks.\n",
        "\n",
        "## Dataset Source and Preprocessing\n",
        "\n",
        "Please refer to the official [ADNI](https://adni.loni.usc.edu/) website for the original dataset."
      ],
      "metadata": {
        "id": "gekhgXdZzGd4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_to_np = lambda img: img.permute(1, 2, 0).cpu().detach().numpy()\n",
        "\n",
        "def denormalize_image(inputs, mean=processor.image_mean, std=processor.image_std):\n",
        "    res = inputs * torch.tensor(std, device=inputs.device)[:, None, None] + torch.tensor(mean, device=inputs.device)[:, None, None]\n",
        "    return img_to_np(res)\n",
        "\n",
        "fig, axes = plt.subplots(1, 4, figsize=(15, 5))\n",
        "\n",
        "label_dict = {0: \"Normal\", 1: \"sMCI\", 2: \"pMCI\", 3: \"AD\"}\n",
        "\n",
        "for i in range(4):\n",
        "    label_idx = dataset[i]['label'].item()\n",
        "    axes[i].imshow(denormalize_image(dataset[i][\"pixel_values\"]))\n",
        "    axes[i].set_title(f\"Label: {label_dict[label_idx]} ({label_idx})\")\n",
        "    axes[i].axis(\"off\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JdtlqLfGxD_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![lear1](./data/lear1.png)"
      ],
      "metadata": {
        "id": "QEJW7teYzGsH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 2.2 Load Pre-trained ResNet Model<a name=\"load-model\"></a>\n",
        "\n",
        "We load a pre-trained ResNet model for Alzheimer's disease classification. The model weights are sourced from the `transformers` library."
      ],
      "metadata": {
        "id": "5ZPE1xaWwZ6g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wyqeuMPbwHq5"
      },
      "outputs": [],
      "source": [
        "class ResNetBase(ResNetForImageClassification):\n",
        "    \"\"\"\n",
        "    PnPXai requires a Tensor output from the model.\n",
        "    We create a custom model since ResNetForImageClassification outputs a tuple[Tensor] instead of a Tensor.\n",
        "    \"\"\"\n",
        "    def forward(self, *args, **kwargs):\n",
        "        return super().forward(*args, **kwargs)[0]\n",
        "\n",
        "model = ResNetBase.from_pretrained(MODEL_REP, return_dict=False).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Explanation Using PnPXAI<a name=\"explanation\"></a>"
      ],
      "metadata": {
        "id": "n6oLNJrdwgeB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "expr = AutoExplanationForImageClassification(\n",
        "    model=model,\n",
        "    data=dataset,\n",
        "    input_extractor=lambda batch: batch['pixel_values'],\n",
        "    label_extractor=lambda batch: batch['label'],\n",
        "    target_extractor=lambda outputs: outputs.argmax(-1).to(device),\n",
        "    target_labels=False,  # target prediction if False\n",
        ")"
      ],
      "metadata": {
        "id": "sj-AgMkZ8dE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 3.1 Add the LEAR Explainer<a name=\"lear-explainer\"></a>\n",
        "\n",
        "We use the `LEAR` explainer from the PnPXAI library to generate explanations for our classification model."
      ],
      "metadata": {
        "id": "7fwXLe2Bwv1M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "> **Learn-Explain-Reinforce: Counterfactual Reasoning and Its Guidance to Reinforce an Alzheimer's Disease Diagnosis Model**<br>\n",
        "> [Kwanseok Oh](https://scholar.google.co.kr/citations?user=EMYHaHUAAAAJ&hl=ko)<sup>1,\\*</sup>, [Jee Seok Yoon](https://scholar.google.co.kr/citations?user=YjaJ5qQAAAAJ&hl=ko)<sup>2,\\*</sup>, and [Heung-Il Suk](https://scholar.google.co.kr/citations?user=dl_oZLwAAAAJ&hl=ko)<sup>1, 2</sup><br/>\n",
        "> (<sup>1</sup>Department of Artificial Intelligence, Korea University) <br/>\n",
        "> (<sup>2</sup>Department of Brain and Cognitive Engineering, Korea University) <br/>\n",
        "> (* indicates equal contribution) <br/>\n",
        "> Official Version: https://ieeexplore.ieee.org/document/9854196 <br/>\n",
        "> Published in IEEE Transactions on Pattern Analysis and Machine Intelligence (2021 JCR-IF: 24.314, COMPUTER SCIENCE & ARTIFICIAL INTELLIGENCE: 2/144)\n",
        "\n",
        "- We propose a novel learn-explain-reinforce framework that integrates the following tasks: (1) training a diagnostic model, (2) explaining a diagnostic model's output, and (3) reinforcing the diagnostic model based on the explanation systematically.\n",
        "- To the best of our knowledge, this work is the first that exploits an explanation output to improve the generalization of a diagnostic model reciprocally.\n",
        "- In regard to explanation, we propose a GAN-based method to produce multi-way counterfactual maps that can provide a more precise explanation, accounting for severity and/or progression of AD.\n",
        "\n",
        "![Group 2896 (3)](https://user-images.githubusercontent.com/57162425/141603646-f714edb2-cc01-4b22-80df-056da791947c.png)\n",
        "\n",
        "\n",
        "### How LEAR’s Loss Functions Shape Its Results\n",
        "1. **$\\mathscr{L}_{\\text{map}}$: Sparsity and Relevance**\n",
        "   - Encourages the counterfactual map to focus only on critical regions (e.g., hippocampus, ventricles) necessary for the transformation.\n",
        "   - Results in clean and targeted explanations, avoiding the scattered and noisy patterns of other methods.\n",
        "\n",
        "2. **$\\mathscr{L}_{\\text{cls}}$: Target-Specific Transformations**\n",
        "   - Ensures that the highlighted regions directly relate to the target condition, providing accurate and meaningful explanations.\n",
        "\n",
        "3. **$\\mathscr{L}_{\\text{adv}}$: Plausibility**\n",
        "   - Enforces the realism of the counterfactual map, ensuring that the transformed image remains anatomically consistent.\n",
        "\n",
        "4. **$\\mathscr{L}_{\\text{cyc}}$: Consistency**\n",
        "   - Guarantees that the transformation between Normal and the target condition is reversible, leading to logical and progressive changes across rows.\n",
        "\n",
        "5. **$\\mathscr{L}_{\\text{tv}}$: Smoothness**\n",
        "   - Minimizes abrupt changes in the counterfactual map, contributing to the clean and noise-free explanations seen in LEAR.\n",
        "\n",
        "In summary, LEAR’s loss functions collectively enable it to produce sparse, targeted, and biologically meaningful visual explanations, setting it apart from traditional methods in both precision and interpretability."
      ],
      "metadata": {
        "id": "1G7YUE0z5_2G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5hMR-HOwjvP"
      },
      "outputs": [],
      "source": [
        "from pnpxai.explainers import LEAR\n",
        "\n",
        "lear_model = LEAR(model=model)\n",
        "expr.manager.add_explainer(lear_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 3.2 Generate Explanations<a name=\"generate-explanations\"></a>\n",
        "\n",
        "We extract explanations for a sample of images.\n"
      ],
      "metadata": {
        "id": "mkw2ex4RwdpU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7zFmUhLwmFQ"
      },
      "outputs": [],
      "source": [
        "explanations = {}\n",
        "postprocessors = {}\n",
        "evaluations = {}\n",
        "\n",
        "for exp_id in range(len(expr.manager.explainers)):\n",
        "    explainer = expr.manager.get_explainer_by_id(exp_id)\n",
        "    postprocessor = expr.manager.get_postprocessor_by_id(exp_id)\n",
        "    name = explainer.__class__.__name__\n",
        "\n",
        "    explanations[name] = []\n",
        "    postprocessors[name] = postprocessor\n",
        "    evaluations[name] = {metric.__class__.__name__: [] for metric in expr.manager.metrics}\n",
        "\n",
        "    for data_idx in range(len(dataset)):\n",
        "        data_sample = dataset[data_idx]\n",
        "        inputs = data_sample['pixel_values'][None]\n",
        "        labels = data_sample['label'][None]\n",
        "        try:\n",
        "            explanations[name] += [torch.concat([explainer.attribute(inputs, torch.full_like(labels, lbl)) for lbl in range(num_classes)],axis=0)]\n",
        "        except NoCamTargetLayerAndNotTraceableError:\n",
        "            explainer = explainer.set_target_layer(model.resnet.encoder)\n",
        "            explanations[name] += [torch.concat([explainer.attribute(inputs, torch.full_like(labels, lbl)) for lbl in range(num_classes)],axis=0)]\n",
        "\n",
        "        # # Note: About 30GB GPU memory is required for the following metric evaluation.\n",
        "        # # If you get out-of-memory errors, comment out the below three lines.\n",
        "        # cur_explanation = explanations[name][data_idx][labels.item()].squeeze()[None]\n",
        "        # for metric in expr.manager.metrics:\n",
        "        #     evaluations[name][metric.__class__.__name__] += [metric.set_explainer(explainer).evaluate(inputs, labels, cur_explanation).item()]\n",
        "\n",
        "\n",
        "    explanations[name] = torch.stack(explanations[name],axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 4. Visualization<a name=\"visualization\"></a>\n"
      ],
      "metadata": {
        "id": "XLYHORoswmkH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Results (presented in the paper)\n",
        "\n",
        ">Visual explanation comparison between XAI methods\n",
        "![img](https://user-images.githubusercontent.com/57162425/141603345-abdf11e0-f7bf-4ecf-979e-f1604cd27c2c.jpg)\n",
        "\n",
        "> Example of counterfactual map conditioned on interpolated target labels\n",
        "\n",
        "<img src=https://user-images.githubusercontent.com/57162425/141603337-4951d4d6-8237-4fc1-80dd-8c87f7dd9d18.png alt=\"drawing\" width=\"500\"/>\n"
      ],
      "metadata": {
        "id": "x22RRChy65vy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Visualize Original Images and Explanations<a name=\"visualize-images\"></a>\n",
        "\n",
        "In this section, we visualize the original images alongside the explanations generated by the `LEAR` explainer."
      ],
      "metadata": {
        "id": "g2WLOqI6VYKF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Key Observations for LEAR\n",
        "The LEAR framework demonstrates the following key properties, attributed to its loss function design:\n",
        "\n",
        "1. **Sparsity and Specificity**:\n",
        "   - LEAR highlights only the most critical regions necessary for transforming the input condition (Normal) into the target condition (e.g., sMCI, pMCI, AD).\n",
        "   - **Key Loss**: The counterfactual map regularization loss ($\\mathscr{L}_{\\text{map}}$) ensures sparsity by suppressing irrelevant regions using $\\ell_1$- and $\\ell_2$-norms.\n",
        "\n",
        "2. **Target-Specific Localization**:\n",
        "   - LEAR focuses on disease-relevant regions, such as early hippocampal changes in sMCI and global atrophy in AD.\n",
        "   - **Key Loss**: The classification loss ($\\mathscr{L}_{\\text{cls}}$) ensures counterfactual maps transform the input toward the target label, emphasizing disease-specific regions.\n",
        "\n",
        "3. **Fine-Grained Detailing**:\n",
        "   - LEAR avoids blurry or noisy regions, producing sharper, more precise explanations compared to methods like VarGrad or Lime.\n",
        "   - **Key Loss**: The adversarial loss ($\\mathscr{L}_{\\text{adv}}^{\\mathscr{G}}$) maintains anatomical plausibility and realism in the maps.\n",
        "\n",
        "4. **Consistency Across Targets**:\n",
        "   - LEAR produces smooth and logical transitions in counterfactual maps (e.g., sMCI → pMCI → AD), aligning with disease progression.\n",
        "   - **Key Loss**: The cycle consistency loss ($\\mathscr{L}_{\\text{cyc}}$) enforces reversible and coherent transformations between conditions.\n",
        "\n",
        "5. **Robustness to Artifacts**:\n",
        "   - LEAR avoids highlighting irrelevant regions (e.g., brain edges), ensuring only biologically meaningful areas are emphasized.\n",
        "   - **Key Loss**: The total variation loss ($\\mathscr{L}_{\\text{tv}}$) reduces abrupt or unnatural changes, improving smoothness and focus.\n",
        "\n",
        "6. **No Changes for \"Normal\" Label**:\n",
        "   - LEAR generates minimal changes for Normal inputs, reflecting logical consistency as no transformation is needed when the input aligns with the target label.\n",
        "\n",
        "7. **Progressive Disease Representation (sMCI → pMCI → AD)**:\n",
        "   - Counterfactual maps reveal increasingly pronounced changes with disease progression:\n",
        "     - **sMCI**: Highlights subtle early changes (e.g., slight hippocampal atrophy).\n",
        "     - **pMCI**: Expands to more pronounced regions of atrophy.\n",
        "     - **AD**: Strongly emphasizes regions like the hippocampus, cortex, and ventricles, reflecting advanced degeneration.\n",
        "\n",
        "8. **Gradual Transformation**:\n",
        "   - LEAR effectively captures smooth and biologically consistent transitions across disease stages, highlighting its ability to model progressive changes."
      ],
      "metadata": {
        "id": "-pKtI-KGV82V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(len(dataset), num_classes + 1, figsize=(12, 10))\n",
        "postprocessor = postprocessors[\"LEAR\"]\n",
        "attributes = explanations[\"LEAR\"]\n",
        "\n",
        "# Display the input image\n",
        "for data_idx in range(len(dataset)):\n",
        "    base_image = denormalize_image(dataset[data_idx]['pixel_values'])\n",
        "    axes[data_idx, 0].imshow(base_image)\n",
        "    axes[data_idx, 0].axis(\"off\")\n",
        "    axes[data_idx, 0].set_title(f\"Input ({label_dict[dataset[data_idx]['label'].item()]})\")\n",
        "\n",
        "    processed_attr = []\n",
        "    # Display explanations\n",
        "    for lbl in range(num_classes):\n",
        "        processed_attr += [postprocessor(attributes[data_idx, lbl][None])[0].cpu().detach().numpy()]\n",
        "    processed_attr = np.array(processed_attr)\n",
        "\n",
        "    # Create an overlay of the processed_attr on the input\n",
        "    overlay = plt.cm.seismic(processed_attr)\n",
        "    mask = (processed_attr >= -0.1) & (processed_attr <= 0.1)\n",
        "    overlay[mask,..., -1] = 0.\n",
        "\n",
        "    for lbl in range(num_classes):\n",
        "        axes[data_idx, lbl + 1].imshow(base_image)\n",
        "        axes[data_idx, lbl + 1].imshow(overlay[lbl])\n",
        "        axes[data_idx, lbl + 1].axis(\"off\")\n",
        "        if data_idx==0:\n",
        "            axes[data_idx, lbl + 1].set_title(f\"{name} ({label_dict[lbl]})\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"ADNI_LEAR.png\", dpi=600)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xYfcUd4NWv_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![lear2](./data/lear2.png)\n",
        "\n",
        "1. **Ventricle Changes (Indicated by Box 1):**\n",
        "    *   In the \"LEAR (Normal)\" image, there is minimal change compared to the input, as expected.\n",
        "    *   As we move towards sMCI, pMCI, and AD, we see a progressive increase in blue coloration in the ventricular region.\n",
        "    *   **Clinical Implication:** Blue indicates regions that need to be *removed* to simulate the target condition. Enlarged ventricles (filled with cerebrospinal fluid) are a known biomarker of Alzheimer's disease and other forms of dementia. The counterfactual maps are highlighting this accurately, suggesting that the model has learned this association.\n",
        "    *   **Technical Implication:** The model is effectively identifying a key structural change associated with disease progression. The increasing intensity of blue from sMCI to pMCI to AD suggests the model can differentiate between disease stages based on ventricular size.\n",
        "\n",
        "2. **Hippocampus Changes (Indicated by Boxes 2, 3, and 4):**\n",
        "    *   In the zoomed-in views (Boxes 2, 3, and 4), we observe blue coloration within the hippocampus, particularly noticeable in the sMCI, pMCI, and AD conditions.\n",
        "    *   **Clinical Implication:** The hippocampus is crucial for memory formation, and its atrophy is a hallmark of Alzheimer's disease. The blue coloration indicates that the model identifies the hippocampus as a region needing reduction to achieve the target (diseased) conditions.\n",
        "    *   **Technical Implication:**  The model has learned to associate hippocampal changes with different stages of cognitive decline. The presence of blue even in the sMCI stage indicates that the model might be picking up on early, subtle changes that could be useful for early detection.\n",
        "\n",
        "3. **Cortical Thinning (Observed throughout the cortex):**\n",
        "    *   There's a subtle but noticeable increase in blue coloration in the cortical regions (the outer layer of the brain) as we move from \"LEAR (Normal)\" to \"LEAR (AD).\"\n",
        "    *   **Clinical Implication:** Cortical thinning is another characteristic of Alzheimer's disease, reflecting neuronal loss and brain atrophy.\n",
        "    *   **Technical Implication:** The model is sensitive to these widespread changes, indicating its ability to capture global patterns of atrophy in addition to localized changes like those in the ventricles and hippocampus.\n",
        "\n",
        "4. **Red Coloration (Observed sparsely):**\n",
        "    *   Red indicates regions that need to be *added* to simulate the target condition. The presence of red in the diseased target conditions (sMCI, pMCI, AD) is minimal, but it can indicate areas where the counterfactual map is suggesting a need for increased tissue volume or density to reflect the target pathology.\n",
        "    *   **Clinical Implication:** It's less common to see increased volume in AD, but these could represent compensatory mechanisms or areas where the model is adjusting for other changes.\n",
        "    *   **Technical Implication:**  The model might be suggesting hypothetical changes that, while not typical, could provide insights into disease progression or individual variations.\n",
        "\n",
        "**Overall Assessment:**\n",
        "\n",
        "The LEAR counterfactual maps appear to be highlighting clinically relevant regions associated with Alzheimer's disease and its prodromal stages. The model seems to have learned to associate specific structural changes (ventricular enlargement, hippocampal atrophy, cortical thinning) with different disease conditions."
      ],
      "metadata": {
        "id": "F3rYwhZ_xBiI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clinical Analysis:\n",
        "1. **Input (Normal):**\n",
        "   - The input image represents a structurally normal brain without visible atrophic or degenerative changes. This serves as the baseline for counterfactual reasoning.\n",
        "\n",
        "2. **LEAR Normal Map:**\n",
        "   - The map for \"Normal\" is **almost blank, signifying minimal or no hypothetical structural changes required** to maintain the brain in its normal state. This aligns with the expectation, as the input is already classified as normal.\n",
        "\n",
        "3. **LEAR sMCI Map:**\n",
        "   - The counterfactual map for **sMCI highlights subtle areas of change**, indicating initial signs of mild cognitive impairment. These changes are localized and minimal, possibly involving slight atrophy or changes in regions such as the hippocampus or cortex.\n",
        "\n",
        "4. **LEAR pMCI Map:**\n",
        "   - The counterfactual map for pMCI exhibits more pronounced regions of change compared to sMCI. These areas likely correspond to progression in neurodegenerative processes, such as **increased atrophy in the medial temporal lobe, ventricles, or cortical regions**. The map shows a potential trajectory from sMCI to a progressive state.\n",
        "\n",
        "5. **LEAR AD Map:**\n",
        "   - The counterfactual map for AD reveals **widespread changes**, likely representing significant atrophy, especially in the hippocampus, entorhinal cortex, and **cortical thinning**. These regions are clinically consistent with advanced Alzheimer’s Disease and align with known biomarkers of AD progression."
      ],
      "metadata": {
        "id": "z9LhcrptrUPr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparison with Other Methods\n",
        "- **Gradient-Based Methods**:\n",
        "  These methods (e.g., Gradient, Gradient × Input) often highlight noisy and dispersed regions, lacking the specificity and disease relevance seen in LEAR. They are susceptible to vanishing gradients and focus on shallow features.\n",
        "- **SHAP and Lime**:\n",
        "  These methods sometimes capture relevant regions but suffer from over-generalization and include extraneous attributions (e.g., Lime’s scattered patterns in the AD row).\n",
        "- **SmoothGrad and VarGrad**:\n",
        "  While these methods reduce noise compared to raw gradient methods, they still struggle to provide fine-grained or localized attributions. Their explanations often highlight broader, less specific areas of the brain, as seen in the diffuse patterns across all rows.\n"
      ],
      "metadata": {
        "id": "exnHeDETw7wp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUcNCZJ9y4Lh"
      },
      "outputs": [],
      "source": [
        "ncols = len(explanations) + 1\n",
        "fig, axes = plt.subplots(len(dataset), ncols, figsize=(18, 5))\n",
        "\n",
        "for data_idx in range(len(dataset)):\n",
        "    lbl = dataset[data_idx]['label'].item()\n",
        "    # Original image\n",
        "    img = denormalize_image(dataset[data_idx]['pixel_values'])\n",
        "    axes[data_idx, 0].imshow(img)\n",
        "    axes[data_idx, 0].axis(\"off\")\n",
        "    axes[data_idx, 0].set_title(f\"Input ({label_dict[lbl]})\", fontsize=7)\n",
        "\n",
        "    # Overlay explanations\n",
        "    for idx, (name, attr) in enumerate(explanations.items(), start=1):\n",
        "        processed_attr = postprocessors[name](attr[lbl])[0].cpu().detach().numpy()\n",
        "        axes[data_idx, idx].imshow(processed_attr, cmap=\"twilight\")\n",
        "        axes[data_idx, idx].axis(\"off\")\n",
        "        if data_idx==0:\n",
        "            axes[data_idx, idx].set_title(f\"{name}\", fontsize=7)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"ADNI_ALL.png\", dpi=600)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![lear3](./data/lear3.png)"
      ],
      "metadata": {
        "id": "kVp25EydzgJb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 5. Evaluation of Explanations<a name=\"evaluation\"></a>\n",
        "\n",
        "### 5.1 MuFidelity<a name=\"mufidelity\"></a>\n",
        "\n",
        "We use `MuFidelity` to evaluate the correctness of the explanations based on input perturbations.\n",
        "\n",
        "### 5.2 Sensitivity<a name=\"sensitivity\"></a>\n",
        "\n",
        "`Sensitivity` measures the robustness of the explanations against input perturbations.\n",
        "\n",
        "### 5.3 Complexity<a name=\"complexity\"></a>\n",
        "\n",
        "The `Complexity` metric is used to evaluate the compactness of the explanations.\n"
      ],
      "metadata": {
        "id": "pu51Z-j0woww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In case you ran out of GPU memory, uncomment the following line to see the evaluation results.\n",
        "evaluations = {'GradCam': {'MuFidelity': [0.5853397928796835, 0.4330805813591715, 0.12873638828392373, 0.19244410862705008], 'AbPC': [0.37591108679771423, 0.3284831941127777, 0.2812458276748657, 0.3626677691936493], 'Sensitivity': [0.08196255564689636, 0.15622945129871368, 0.07326357066631317, 0.11029939353466034], 'Complexity': [1.3829326954758148, 1.7525320283806376, 1.330464282607583, 1.5904362652105362]}, 'Gradient': {'MuFidelity': [0.22466598515489578, 0.16230943597493225, -0.016343837503889062, -0.2452926796746522], 'AbPC': [0.00018052756786346436, 2.3117907588243725e-08, -0.03194704279303551, 0.10668325424194336], 'Sensitivity': [0.7811529040336609, 0.7766968607902527, 0.7730032801628113, 0.733552098274231], 'Complexity': [0.9188669536600583, 0.9892004914913868, 0.8657636078024417, 0.545632421698694]}, 'GradientXInput': {'MuFidelity': [0.12386150495577582, 0.1414978443486377, -0.21093026356726965, 0.06160807146984311], 'AbPC': [5.334007013857445e-08, 2.216229699758543e-10, 0.020592985674738884, 0.07454539835453033], 'Sensitivity': [0.8574092984199524, 0.8124198913574219, 0.7832929491996765, 0.8289350867271423], 'Complexity': [0.4640524469258037, 0.8975054238854383, 0.5570392715446303, 0.5316761437434923]}, 'GuidedGradCam': {'MuFidelity': [-0.3399208853726832, -0.4091186274945553, -0.08631316947419886, -0.2241930752477888], 'AbPC': [0.004187882877886295, -1.945003714354243e-05, 0.0592629611492157, -0.4073661267757416], 'Sensitivity': [0.3443097174167633, 0.42247045040130615, 0.28653573989868164, 0.27038756012916565], 'Complexity': [0.15383228052953557, 0.4102344343626338, 0.12933598315008693, 0.06571942795199591]}, 'IntegratedGradients': {'MuFidelity': [0.11060580470243124, 0.25043424152184546, -0.20757722565447354, 0.17588870616471844], 'AbPC': [7.333567282330478e-07, 2.0256629706949525e-09, 0.0156655665487051, -0.03121274709701538], 'Sensitivity': [0.6874512871593792, 0.7007013623496835, 0.6781195081621901, 0.6951203379943444], 'Complexity': [0.7968953879823089, 0.7030053462299225, 0.5867182052496104, 0.5087089242698335]}, 'KernelShap': {'MuFidelity': [-0.014466420729810215, -0.1578576825636695, -0.1868580825814481, -0.010668918618605272], 'AbPC': [-0.005147154442965984, -0.0003402233705855906, -0.0012969732051715255, 0.002396901836618781], 'Sensitivity': [1.4471503496170044, 1.3551380634307861, 1.4879704713821411, 1.4673168659210205], 'Complexity': [1.8898915191165568, 2.025259025707154, 1.7585286180793283, 1.8720605431322936]}, 'LRPEpsilonAlpha2Beta1': {'MuFidelity': [-0.24062758344815324, -0.45509044846437624, -0.11095782034757101, -0.26090226232276986], 'AbPC': [0.08782243728637695, 6.970660137994855e-08, 0.0011385142570361495, 0.03893474489450455], 'Sensitivity': [0.3933000862598419, 0.34724971652030945, 0.39051833748817444, 0.45043662190437317], 'Complexity': [0.24262569322009134, 0.13388834989827372, 0.11189638457094797, 0.11859441850116519]}, 'LRPEpsilonGammaBox': {'MuFidelity': [0.32818703053469045, 0.5859549313302814, 0.09149028845726477, 0.3405075781145829], 'AbPC': [0.20114457607269287, -3.2178346032196714e-07, -0.16582301259040833, 0.23006954789161682], 'Sensitivity': [0.25641143321990967, 0.3286450207233429, 0.2549888789653778, 0.2653220593929291], 'Complexity': [0.38584132454145315, 0.22832062534792646, 0.3089214065583735, 0.6554900430973418]}, 'LRPEpsilonPlus': {'MuFidelity': [0.5020152006755857, 0.600419574203298, 0.07926574514422864, 0.06218054135739367], 'AbPC': [0.38250207901000977, 0.26585373282432556, 0.2119503766298294, 0.327301949262619], 'Sensitivity': [0.2060757577419281, 0.2129177302122116, 0.2091846615076065, 0.1984558403491974], 'Complexity': [0.6075966301552908, 0.4844051378757354, 0.3125432155643451, 0.2150304209888569]}, 'LRPUniformEpsilon': {'MuFidelity': [-0.2955882483665941, -0.3968834170407574, 0.12718965287346104, -0.348091915196231], 'AbPC': [-0.02581048011779785, -3.133705517299745e-10, 0.014265470206737518, -0.06685128062963486], 'Sensitivity': [0.749815046787262, 0.7304776310920715, 0.7322759032249451, 0.7061777710914612], 'Complexity': [1.1550005083980674, 0.9798314565473537, 1.0342592676552143, 0.8451126437100633]}, 'Lime': {'MuFidelity': [-0.19123161029379085, -0.1152211209387084, -0.05028312369438642, 0.04985643806391395], 'AbPC': [-0.25141197443008423, 0.09014959633350372, -8.714794967090711e-05, -0.15750978887081146], 'Sensitivity': [4.3532233238220215, 1.7156202793121338, 3.960479497909546, 2.6730077266693115], 'Complexity': [1.1726298866284242, 1.0184589741556143, 1.004482578136322, 0.7726287539665619]}, 'SmoothGrad': {'MuFidelity': [0.22171474287746123, 0.11053824614427307, -0.05829770212009423, -0.2939277301213387], 'AbPC': [0.007186622358858585, 2.446516589316161e-07, -0.10587692260742188, 0.6451295018196106], 'Sensitivity': [0.5690613985061646, 0.5328739881515503, 0.5173720717430115, 0.5148533582687378], 'Complexity': [0.8364464764354549, 0.7181471006727972, 0.729735989359631, 0.4057498493427954]}, 'VarGrad': {'MuFidelity': [-0.22961553846837637, -0.07504866882972576, -0.002266767411884973, 0.17583892617449667], 'AbPC': [-0.041456807404756546, -2.7459461193757306e-07, 0.18327400088310242, -0.6636568903923035], 'Sensitivity': [0.5782389044761658, 0.5332300662994385, 0.5200970768928528, 0.5021266341209412], 'Complexity': [0.7316020043213131, 0.7963895301583934, 0.5259080445187403, 0.390640311565816]}, 'LEAR': {'MuFidelity': [0.11926752300102228, 0.1393395262011645, 0.05074536646073159, 0.07323525490021779], 'AbPC': [0.3547067642211914, 0.15967178344726562, 0.23337431252002716, 0.20952379703521729], 'Sensitivity': [1.302302360534668, 1.3157992362976074, 1.151240587234497, 1.239722728729248], 'Complexity': [0.052206083728354696, 0.03326454445101344, 0.023126950677495157, 0.021252525662937338]}}\n",
        "\n",
        "# Calculate average values for each metric and method\n",
        "averages = {}\n",
        "for method, metrics in evaluations.items():\n",
        "    averages[method] = {}\n",
        "    for metric, values in metrics.items():\n",
        "        averages[method][metric] = np.mean(values)\n",
        "\n",
        "# Prepare data for plotting\n",
        "methods = list(averages.keys())\n",
        "metrics = list(evaluations[methods[0]].keys())  # Get the metrics from the first method\n",
        "bar_width = 0.2\n",
        "index = np.arange(len(methods))\n",
        "\n",
        "# Create the bar chart\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "for i, metric in enumerate(metrics):\n",
        "    metric_averages = [averages[method][metric] for method in methods]\n",
        "    ax.bar(index + i * bar_width, metric_averages, bar_width, label=metric)\n",
        "\n",
        "# Customize the plot\n",
        "ax.set_ylabel('Average Metric Value')\n",
        "ax.set_title('Average Values of Quantitative Metrics Across Explanation Methods')\n",
        "ax.set_xticks(index + bar_width * (len(metrics) - 1) / 2)\n",
        "ax.set_xticklabels(methods, rotation=90)\n",
        "ax.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "9w8xQm_DmZF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![lear4](./data/lear4.png)"
      ],
      "metadata": {
        "id": "xEwKcMUt8pUA"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyNzSluKMebEEI5alQrA9N/v",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}